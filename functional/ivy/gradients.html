<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Gradients &mdash; Ivy 1.0 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="icon" type="image/png" href="https://github.com/unifyai/unifyai.github.io/blob/master/img/externally_linked/ivy_logo_only.png?raw=true">
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="with_grads" href="gradients/with_grads.html" />
    <link rel="prev" title="arg_info" href="general/arg_info.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Ivy
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../background.html">Background</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design.html">Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../extensions.html">Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../roadmap.html">Roadmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep_dive.html">Deep Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Functions</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="activations.html">Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="compilation.html">Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="constants.html">Constants</a></li>
<li class="toctree-l1"><a class="reference internal" href="creation.html">Creation</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_type.html">Data type</a></li>
<li class="toctree-l1"><a class="reference internal" href="device.html">Device</a></li>
<li class="toctree-l1"><a class="reference internal" href="elementwise.html">Elementwise</a></li>
<li class="toctree-l1"><a class="reference internal" href="general.html">General</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Gradients</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gradients/with_grads.html">with_grads</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/set_with_grads.html">set_with_grads</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/unset_with_grads.html">unset_with_grads</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/variable.html">variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/is_variable.html">is_variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/variable_data.html">variable_data</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/stop_gradient.html">stop_gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/execute_with_gradients.html">execute_with_gradients</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/adam_step.html">adam_step</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/optimizer_update.html">optimizer_update</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/gradient_descent_update.html">gradient_descent_update</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/lars_update.html">lars_update</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/adam_update.html">adam_update</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/lamb_update.html">lamb_update</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_algebra.html">Linear algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="losses.html">Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="manipulation.html">Manipulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">Meta</a></li>
<li class="toctree-l1"><a class="reference internal" href="nest.html">Nest</a></li>
<li class="toctree-l1"><a class="reference internal" href="norms.html">Norms</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">Random</a></li>
<li class="toctree-l1"><a class="reference internal" href="searching.html">Searching</a></li>
<li class="toctree-l1"><a class="reference internal" href="set.html">Set</a></li>
<li class="toctree-l1"><a class="reference internal" href="sorting.html">Sorting</a></li>
<li class="toctree-l1"><a class="reference internal" href="statistical.html">Statistical</a></li>
<li class="toctree-l1"><a class="reference internal" href="utility.html">Utility</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Stateful</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/activations.html">Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/initializers.html">Initializers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/module.html">Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/norms.html">Norms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/optimizers.html">Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/sequential.html">Sequential</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../ivy"">Ivy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mech"">Ivy mech</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../vision"">Ivy vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../robot"">Ivy robot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gym"">Ivy gym</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../memory"">Ivy memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../builder"">Ivy builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../models"">Ivy models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ecosystem"">Ivy ecosystem</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Ivy</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
                <li>Gradients</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/functional/ivy/gradients.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-ivy.functional.ivy.gradients">
<span id="gradients"></span><h1>Gradients<a class="headerlink" href="#module-ivy.functional.ivy.gradients" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>Collection of gradient Ivy functions.</p>
<dl class="py class">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.GradientTracking">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">GradientTracking</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">with_grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#GradientTracking"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.GradientTracking" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.GradientTracking.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">with_grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#GradientTracking.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.GradientTracking.__init__" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.adam_step">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">adam_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dcdw</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mw</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vw</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#adam_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.adam_step" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Compute adam step delta, given the derivatives of some cost c with respect
to weights ws, using ADAM update. <a href="#id1"><span class="problematic" id="id2">`</span></a>[reference]</p>
<p>&lt;<a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam">https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam</a>&gt;`_</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dcdw</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>mw</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì running average of the gradients</p></li>
<li><p><strong>vw</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì running average of second moments of the gradients</p></li>
<li><p><strong>step</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) ‚Äì training step</p></li>
<li><p><strong>beta1</strong> ‚Äì gradient forgetting factor (Default value = 0.9)</p></li>
<li><p><strong>beta2</strong> ‚Äì second moment of gradient forgetting factor (Default value = 0.999)</p></li>
<li><p><strong>epsilon</strong> ‚Äì divisor during adam update, preventing division by zero (Default value = 1e-7)</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><em>ret</em> ‚Äì The adam step delta.</p>
</dd>
</dl>
<p class="rubric">Functional Examples</p>
<p>With <code class="code docutils literal notranslate"><span class="pre">ivy.Array</span></code> inputs:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dcdw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">step</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">adam_step_delta</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">adam_step</span><span class="p">(</span><span class="n">dcdw</span><span class="p">,</span> <span class="n">mw</span><span class="p">,</span> <span class="n">vw</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">adam_step_delta</span><span class="p">)</span>
<span class="go">    (ivy.array([0.639, 0.639, 0.639]),</span>
<span class="go">    ivy.array([0.1, 0.2, 0.3]),</span>
<span class="go">    ivy.array([0.001, 0.004, 0.009]))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dcdw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">step</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.86</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">adam_step_delta</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">adam_step</span><span class="p">(</span><span class="n">dcdw</span><span class="p">,</span> <span class="n">mw</span><span class="p">,</span> <span class="n">vw</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">adam_step_delta</span><span class="p">)</span>
<span class="go">    (ivy.array([[1., 1., -1.],</span>
<span class="go">                [1., 1., 1.]]),</span>
<span class="go">    ivy.array([[ 0.14, 0.56, -0.42],</span>
<span class="go">              [ 0.28, 0.42, 0.07]]),</span>
<span class="go">    ivy.array([[0.05, 0.8, 0.45],</span>
<span class="go">              [0.2, 0.45, 0.0125]]))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dcdw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">step</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">3.6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">adam_step_delta</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">adam_step</span><span class="p">(</span><span class="n">dcdw</span><span class="p">,</span> <span class="n">mw</span><span class="p">,</span> <span class="n">vw</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">adam_step_delta</span><span class="p">)</span>
<span class="go">    (ivy.array([ 0.601, -0.601, 0.601]),</span>
<span class="go">    ivy.array([ 0.1, -0.2, 0.3]),</span>
<span class="go">    ivy.array([0.001, 0.004, 0.009]))</span>
</pre></div>
</div>
<p>With <code class="code docutils literal notranslate"><span class="pre">ivy.NativeArray</span></code> inputs:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dcdw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">native_array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">native_array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">native_array</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">step</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">native_array</span><span class="p">([</span><span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">adam_step_delta</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">adam_step</span><span class="p">(</span><span class="n">dcdw</span><span class="p">,</span> <span class="n">mw</span><span class="p">,</span> <span class="n">vw</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">adam_step_delta</span><span class="p">)</span>
<span class="go">    (ivy.array([0.581, 0.581, 0.581]),</span>
<span class="go">    ivy.array([0.2, 0.3, 0.5]),</span>
<span class="go">    ivy.array([0.004, 0.009, 0.025]))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dcdw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">native_array</span><span class="p">([</span><span class="mf">3.</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">7</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">native_array</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">step</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">native_array</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.76</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.992</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">adam_step_delta</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">adam_step</span><span class="p">(</span><span class="n">dcdw</span><span class="p">,</span> <span class="n">mw</span><span class="p">,</span> <span class="n">vw</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">adam_step_delta</span><span class="p">)</span>
<span class="go">    (ivy.array([0.209, -0.271, 0.0717, 0., 0.142, -0.209, 0.182]),</span>
<span class="go">     ivy.array([ 0.72, -0.96, 0.24, 0., 0.48, -0.72, 0.624]),</span>
<span class="go">     ivy.array([1.06, 1.12, 1., 0.992, 1.02, 1.06, 1.05]))</span>
</pre></div>
</div>
<p>with mixture of both <code class="code docutils literal notranslate"><span class="pre">ivy.NativeArray</span></code>  and :code:‚Äôivy.Array‚Äô inputs:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dcdw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">native_array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">step</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">native_array</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">adam_step_delta</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">adam_step</span><span class="p">(</span><span class="n">dcdw</span><span class="p">,</span> <span class="n">mw</span><span class="p">,</span> <span class="n">vw</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">adam_step_delta</span><span class="p">)</span>
<span class="go">    (ivy.array([0.744, 0.744, 0.744]),</span>
<span class="go">    ivy.array([0.1, 0.2, 0.3]),</span>
<span class="go">    ivy.array([0.001, 0.004, 0.009]))</span>
</pre></div>
</div>
<p>with :code: <cite>ivy.container</cite> inputs:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dcdw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">Container</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">ivy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]),</span>                             <span class="n">b</span><span class="o">=</span><span class="n">ivy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">Container</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">ivy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]),</span>                           <span class="n">b</span><span class="o">=</span><span class="n">ivy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vw</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">Container</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">ivy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,]),</span>                           <span class="n">b</span><span class="o">=</span><span class="n">ivy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">step</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.87</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.976</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">adam_step_delta</span> <span class="o">=</span> <span class="n">ivy</span><span class="o">.</span><span class="n">adam_step</span><span class="p">(</span><span class="n">dcdw</span><span class="p">,</span> <span class="n">mw</span><span class="p">,</span> <span class="n">vw</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">adam_step_delta</span><span class="p">)</span>
<span class="go">({</span>
<span class="go">    a: ivy.array([0., 0.626, 0.626]),</span>
<span class="go">    b: ivy.array([0.626, 0.626, 0.626])</span>
<span class="go">}, {</span>
<span class="go">    a: ivy.array([0., 0.13, 0.26]),</span>
<span class="go">    b: ivy.array([0.39, 0.52, 0.65])</span>
<span class="go">}, {</span>
<span class="go">    a: ivy.array([0., 0.024, 0.096]),</span>
<span class="go">    b: ivy.array([0.216, 0.384, 0.6])</span>
<span class="go">})</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.adam_update">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">adam_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">w</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dcdw</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mw_tm1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vw_tm1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_gradients</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#adam_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.adam_update" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Update weights ws of some function, given the derivatives of some cost c with
respect to ws, using ADAM update. <a href="#id3"><span class="problematic" id="id4">`</span></a>[reference]</p>
<p>&lt;<a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam">https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam</a>&gt;`_</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>w</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Weights of the function to be updated.</p></li>
<li><p><strong>dcdw</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Learning rate(s), the rate(s) at which the weights should be updated relative to
the gradient.</p></li>
<li><p><strong>mw_tm1</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì running average of the gradients, from the previous time-step.</p></li>
<li><p><strong>vw_tm1</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì running average of second moments of the gradients, from the previous time-step.</p></li>
<li><p><strong>step</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì training step</p></li>
<li><p><strong>beta1</strong> ‚Äì gradient forgetting factor (Default value = 0.9)</p></li>
<li><p><strong>beta2</strong> ‚Äì second moment of gradient forgetting factor (Default value = 0.999)</p></li>
<li><p><strong>epsilon</strong> ‚Äì divisor during adam update, preventing division by zero (Default value = 1e-7)</p></li>
<li><p><strong>inplace</strong> ‚Äì Whether to perform the operation inplace, for backends which support inplace
variable updates, and handle gradients behind the scenes such as PyTorch. If the
update step should form part of a computation graph (i.e. higher order
optimization), then this should be set to False. Default is True, provided the
backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> ‚Äì Whether to stop the gradients of the variables after each gradient step.
Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><em>ret</em> ‚Äì The new function weights ws_new, and also new mw and vw, following the adam
updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.execute_with_gradients">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">execute_with_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">func</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">xs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retain_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#execute_with_gradients"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.execute_with_gradients" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Call function func with input of xs variables, and return func first output y,
the gradients [dy/dx for x in xs], and any other function outputs after the returned
y value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> ‚Äì Function for which we compute the gradients of the output with respect to xs
input.</p></li>
<li><p><strong>xs</strong> ‚Äì Variables for which to compute the function gradients with respective to.</p></li>
<li><p><strong>retain_grads</strong> ‚Äì Whether to retain the gradients of the returned values. (Default value = False)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>ret</em> ‚Äì the function first output y, the gradients [dy/dx for x in xs], and any other
extra function outputs</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.gradient_descent_update">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">gradient_descent_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">w</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dcdw</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_gradients</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#gradient_descent_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.gradient_descent_update" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Update weights ws of some function, given the derivatives of some cost c with
respect to ws, [dc/dw for w in ws].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>w</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Weights of the function to be updated.</p></li>
<li><p><strong>dcdw</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Learning rate(s), the rate(s) at which the weights should be updated relative to
the gradient.</p></li>
<li><p><strong>inplace</strong> ‚Äì Whether to perform the operation inplace, for backends which support inplace
variable updates, and handle gradients behind the scenes such as PyTorch. If the
update step should form part of a computation graph (i.e. higher order
optimization), then this should be set to False. Default is True, provided the
backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> ‚Äì Whether to stop the gradients of the variables after each gradient step.
Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><em>ret</em> ‚Äì The new function weights ws_new, following the gradient descent updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.is_variable">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">is_variable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclusive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#is_variable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.is_variable" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Determines whether the input is a variable or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> ‚Äì An ivy array.</p></li>
<li><p><strong>exclusive</strong> ‚Äì Whether to check if the data type is exclusively a variable, rather than an
array. For frameworks like JAX that do not have exclusive variable types, the
function will always return False if this flag is set, otherwise the check is
the same for general arrays. Default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>ret</em> ‚Äì Boolean, true if x is a trainable variable, false otherwise.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.lamb_update">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">lamb_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">w</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dcdw</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mw_tm1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vw_tm1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_trust_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decay_lambda</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_gradients</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#lamb_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.lamb_update" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Update weights ws of some function, given the derivatives of some cost c with
respect to ws, [dc/dw for w in ws], by applying LAMB method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>w</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Weights of the function to be updated.</p></li>
<li><p><strong>dcdw</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Learning rate(s), the rate(s) at which the weights should be updated relative to
the gradient.</p></li>
<li><p><strong>mw_tm1</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì running average of the gradients, from the previous time-step.</p></li>
<li><p><strong>vw_tm1</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì running average of second moments of the gradients, from the previous time-step.</p></li>
<li><p><strong>step</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì training step</p></li>
<li><p><strong>beta1</strong> ‚Äì gradient forgetting factor (Default value = 0.9)</p></li>
<li><p><strong>beta2</strong> ‚Äì second moment of gradient forgetting factor (Default value = 0.999)</p></li>
<li><p><strong>epsilon</strong> ‚Äì divisor during adam update, preventing division by zero (Default value = 1e-7)</p></li>
<li><p><strong>max_trust_ratio</strong> ‚Äì The maximum value for the trust ratio. Default is 10.</p></li>
<li><p><strong>decay_lambda</strong> ‚Äì The factor used for weight decay. Default is zero.</p></li>
<li><p><strong>inplace</strong> ‚Äì Whether to perform the operation inplace, for backends which support inplace
variable updates, and handle gradients behind the scenes such as PyTorch. If the
update step should form part of a computation graph (i.e. higher order
optimization), then this should be set to False. Default is True, provided the
backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> ‚Äì Whether to stop the gradients of the variables after each gradient step.
Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><em>ret</em> ‚Äì The new function weights ws_new, following the LARS updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.lars_update">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">lars_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">w</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dcdw</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decay_lambda</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_gradients</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#lars_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.lars_update" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Update weights ws of some function, given the derivatives of some cost c with
respect to ws, [dc/dw for w in ws], by applying Layerwise Adaptive Rate Scaling
(LARS) method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>w</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Weights of the function to be updated.</p></li>
<li><p><strong>dcdw</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Learning rate, the rate at which the weights should be updated relative to the
gradient.</p></li>
<li><p><strong>decay_lambda</strong> ‚Äì The factor used for weight decay. Default is zero.</p></li>
<li><p><strong>inplace</strong> ‚Äì Whether to perform the operation inplace, for backends which support inplace
variable updates, and handle gradients behind the scenes such as PyTorch. If the
update step should form part of a computation graph (i.e. higher order
optimization), then this should be set to False. Default is True, provided the
backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> ‚Äì Whether to stop the gradients of the variables after each gradient step.
Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><em>ret</em> ‚Äì The new function weights ws_new, following the LARS updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.optimizer_update">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">optimizer_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">w</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">effective_grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_gradients</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#optimizer_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.optimizer_update" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Update weights ws of some function, given the true or effective derivatives of
some cost c with respect to ws, [dc/dw for w in ws].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>w</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Weights of the function to be updated.</p></li>
<li><p><strong>effective_grad</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Effective gradients of the cost c with respect to the weights ws,
[dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Learning rate(s), the rate(s) at which the weights should be updated relative to
the gradient.</p></li>
<li><p><strong>inplace</strong> ‚Äì Whether to perform the operation inplace, for backends which support inplace
variable updates, and handle gradients behind the scenes such as PyTorch. If the
update step should form part of a computation graph (i.e. higher order
optimization), then this should be set to False. Default is True, provided the
backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> ‚Äì Whether to stop the gradients of the variables after each gradient step.
Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><em>ret</em> ‚Äì The new function weights ws_new, following the optimizer updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.set_with_grads">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">set_with_grads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">with_grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#set_with_grads"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.set_with_grads" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Summary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>with_grads</strong> ‚Äì </p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.stop_gradient">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">stop_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preserve_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#stop_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.stop_gradient" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Stops gradient computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì Array for which to stop the gradient.</p></li>
<li><p><strong>preserve_type</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) ‚Äì Whether to preserve the input type (ivy.Variable or ivy.Array), (default: <code class="docutils literal notranslate"><span class="pre">True</span></code>)
otherwise an array is always returned. Default is True.</p></li>
<li><p><strong>preserve_type</strong> ‚Äì bool, optional (Default value = True)</p></li>
<li><p><strong>out</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>]) ‚Äì optional output array, for writing the result to. It must have a shape that the (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)
inputs broadcast to.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><em>ret</em> ‚Äì The same array x, but with no gradient information.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.unset_with_grads">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">unset_with_grads</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#unset_with_grads"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.unset_with_grads" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.variable">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">variable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#variable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.variable" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Creates a variable, which supports gradient computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì An ivy array.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><em>ret</em> ‚Äì An ivy variable, supporting gradient computation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.variable_data">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">variable_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#variable_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.variable_data" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Some backends wrap arrays in a dedicated variable class. For those frameworks,
this function returns that wrapped array. For frameworks which do not have a
dedicated variable class, the function returns the data passed in.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> ‚Äì An ivy variable.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>ret</em> ‚Äì The internal data stored by the variable</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.with_grads">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">with_grads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">with_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#with_grads"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.with_grads" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Enter a nested code space where gradients are computed. This method
adds the with_grads component to the global list with_grads_stack</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>with_grads</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) ‚Äì Boolean value denoting whether the current code block has gradient (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)
computation enabled or not.
‚ÄòTrue‚Äô or ‚ÄòFalse‚Äô or ‚ÄòNone‚Äô (Default value = None)</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><em>ret</em> ‚Äì If with_grads is boolean, it returns the boolean value representing
if gradient computation is enabled or not.
If with_grads is None, it returns the last element in the with_grads_stack
representing the parent of the current nested code block. If with_grads_stack
is empty, it returns True by default.
If with_grads is neither None nor boolean, it will raise an AssertionError</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ivy</span><span class="o">.</span><span class="n">set_with_grads</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ivy</span><span class="o">.</span><span class="n">with_grads</span><span class="p">(</span><span class="n">with_grads</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ivy</span><span class="o">.</span><span class="n">set_with_grads</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ivy</span><span class="o">.</span><span class="n">with_grads</span><span class="p">(</span><span class="n">with_grads</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="go">False</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ivy</span><span class="o">.</span><span class="n">with_grads</span><span class="p">(</span><span class="n">with_grads</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ivy</span><span class="o">.</span><span class="n">with_grads</span><span class="p">(</span><span class="n">with_grads</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="go">False</span>
</pre></div>
</div>
</dd></dl>

<div class="toctree-wrapper compound">
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="general/arg_info.html" class="btn btn-neutral float-left" title="arg_info" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="gradients/with_grads.html" class="btn btn-neutral float-right" title="with_grads" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2022, Ivy Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>