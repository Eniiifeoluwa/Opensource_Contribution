<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Gradients &mdash; Ivy 1.0 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="icon" type="image/png" href="https://github.com/unifyai/unifyai.github.io/blob/master/img/externally_linked/ivy_logo_only.png?raw=true">
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="with_grads" href="gradients/with_grads.html" />
    <link rel="prev" title="get_num_dims" href="general/get_num_dims.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Ivy
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../background.html">Background</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design.html">Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../extensions.html">Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../roadmap.html">Roadmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep_dive.html">Deep Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Functions</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="activations.html">Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="compilation.html">Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="constants.html">Constants</a></li>
<li class="toctree-l1"><a class="reference internal" href="creation.html">Creation</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_type.html">Data type</a></li>
<li class="toctree-l1"><a class="reference internal" href="device.html">Device</a></li>
<li class="toctree-l1"><a class="reference internal" href="elementwise.html">Elementwise</a></li>
<li class="toctree-l1"><a class="reference internal" href="general.html">General</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Gradients</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gradients/with_grads.html">with_grads</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/set_with_grads.html">set_with_grads</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/unset_with_grads.html">unset_with_grads</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/variable.html">variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/is_variable.html">is_variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/variable_data.html">variable_data</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/stop_gradient.html">stop_gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/execute_with_gradients.html">execute_with_gradients</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/adam_step.html">adam_step</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/optimizer_update.html">optimizer_update</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/gradient_descent_update.html">gradient_descent_update</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/lars_update.html">lars_update</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/adam_update.html">adam_update</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradients/lamb_update.html">lamb_update</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="image.html">Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_algebra.html">Linear algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="losses.html">Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="manipulation.html">Manipulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">Meta</a></li>
<li class="toctree-l1"><a class="reference internal" href="nest.html">Nest</a></li>
<li class="toctree-l1"><a class="reference internal" href="norms.html">Norms</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">Random</a></li>
<li class="toctree-l1"><a class="reference internal" href="searching.html">Searching</a></li>
<li class="toctree-l1"><a class="reference internal" href="set.html">Set</a></li>
<li class="toctree-l1"><a class="reference internal" href="sorting.html">Sorting</a></li>
<li class="toctree-l1"><a class="reference internal" href="statistical.html">Statistical</a></li>
<li class="toctree-l1"><a class="reference internal" href="utility.html">Utility</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Stateful</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/activations.html">Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/converters.html">Converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/initializers.html">Initializers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/module.html">Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/norms.html">Norms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/optimizers.html">Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stateful/sequential.html">Sequential</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../ivy"">Ivy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mech"">Ivy mech</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../vision"">Ivy vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../robot"">Ivy robot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gym"">Ivy gym</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../memory"">Ivy memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../builder"">Ivy builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../models"">Ivy models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ecosystem"">Ivy ecosystem</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Ivy</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
                <li>Gradients</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/functional/ivy/gradients.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-ivy.functional.ivy.gradients">
<span id="gradients"></span><h1>Gradients<a class="headerlink" href="#module-ivy.functional.ivy.gradients" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>Collection of gradient Ivy functions.</p>
<dl class="py class">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.GradientTracking">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">GradientTracking</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">with_grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#GradientTracking"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.GradientTracking" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.GradientTracking.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">with_grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#GradientTracking.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.GradientTracking.__init__" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.adam_step">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">adam_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dcdws</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mw</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vw</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#adam_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.adam_step" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Compute adam step delta, given the derivatives of some cost c with respect to ws,
using ADAM update. <a href="#id1"><span class="problematic" id="id2">`</span></a>[reference]</p>
<p>&lt;<a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam">https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam</a>&gt;`_</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dcdws</strong> ‚Äì Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>mw</strong> ‚Äì running average of the gradients</p></li>
<li><p><strong>vw</strong> ‚Äì running average of second moments of the gradients</p></li>
<li><p><strong>step</strong> ‚Äì training step</p></li>
<li><p><strong>beta1</strong> ‚Äì gradient forgetting factor (Default value = 0.9)</p></li>
<li><p><strong>beta2</strong> ‚Äì second moment of gradient forgetting factor (Default value = 0.999)</p></li>
<li><p><strong>epsilon</strong> ‚Äì divisor during adam update, preventing division by zero (Default value = 1e-7)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>ret</em> ‚Äì The adam step delta.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.adam_update">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">adam_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ws</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dcdws</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mw_tm1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vw_tm1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_gradients</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#adam_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.adam_update" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Update weights ws of some function, given the derivatives of some cost c with
respect to ws, using ADAM update. <a href="#id3"><span class="problematic" id="id4">`</span></a>[reference]</p>
<p>&lt;<a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam">https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam</a>&gt;`_</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ws</strong> ‚Äì Weights of the function to be updated.</p></li>
<li><p><strong>dcdws</strong> ‚Äì Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> ‚Äì Learning rate(s), the rate(s) at which the weights should be updated relative to
the gradient.</p></li>
<li><p><strong>mw_tm1</strong> ‚Äì running average of the gradients, from the previous time-step.</p></li>
<li><p><strong>vw_tm1</strong> ‚Äì running average of second moments of the gradients, from the previous time-step.</p></li>
<li><p><strong>step</strong> ‚Äì training step</p></li>
<li><p><strong>beta1</strong> ‚Äì gradient forgetting factor (Default value = 0.9)</p></li>
<li><p><strong>beta2</strong> ‚Äì second moment of gradient forgetting factor (Default value = 0.999)</p></li>
<li><p><strong>epsilon</strong> ‚Äì divisor during adam update, preventing division by zero (Default value = 1e-7)</p></li>
<li><p><strong>inplace</strong> ‚Äì Whether to perform the operation inplace, for backends which support inplace
variable updates, and handle gradients behind the scenes such as PyTorch. If the
update step should form part of a computation graph (i.e. higher order
optimization), then this should be set to False. Default is True, provided the
backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> ‚Äì Whether to stop the gradients of the variables after each gradient step.
Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>ret</em> ‚Äì The new function weights ws_new, and also new mw and vw, following the adam
updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.execute_with_gradients">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">execute_with_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">func</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">xs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retain_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#execute_with_gradients"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.execute_with_gradients" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Call function func with input of xs variables, and return func first output y,
the gradients [dy/dx for x in xs], and any other function outputs after the returned
y value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> ‚Äì Function for which we compute the gradients of the output with respect to xs
input.</p></li>
<li><p><strong>xs</strong> ‚Äì Variables for which to compute the function gradients with respective to.</p></li>
<li><p><strong>retain_grads</strong> ‚Äì Whether to retain the gradients of the returned values. (Default value = False)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>ret</em> ‚Äì the function first output y, the gradients [dy/dx for x in xs], and any other
extra function outputs</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.gradient_descent_update">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">gradient_descent_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ws</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dcdws</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_gradients</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#gradient_descent_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.gradient_descent_update" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Update weights ws of some function, given the derivatives of some cost c with
respect to ws, [dc/dw for w in ws].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ws</strong> ‚Äì Weights of the function to be updated.</p></li>
<li><p><strong>dcdws</strong> ‚Äì Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> ‚Äì Learning rate(s), the rate(s) at which the weights should be updated relative to
the gradient.</p></li>
<li><p><strong>inplace</strong> ‚Äì Whether to perform the operation inplace, for backends which support inplace
variable updates, and handle gradients behind the scenes such as PyTorch. If the
update step should form part of a computation graph (i.e. higher order
optimization), then this should be set to False. Default is True, provided the
backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> ‚Äì Whether to stop the gradients of the variables after each gradient step.
Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>ret</em> ‚Äì The new function weights ws_new, following the gradient descent updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.is_variable">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">is_variable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclusive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#is_variable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.is_variable" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Determines whether the input is a variable or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> ‚Äì An ivy array.</p></li>
<li><p><strong>exclusive</strong> ‚Äì Whether to check if the data type is exclusively a variable, rather than an
array. For frameworks like JAX that do not have exclusive variable types, the
function will always return False if this flag is set, otherwise the check is
the same for general arrays. Default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>ret</em> ‚Äì Boolean, true if x is a trainable variable, false otherwise.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.lamb_update">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">lamb_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ws</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dcdws</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mw_tm1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vw_tm1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_trust_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decay_lambda</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_gradients</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#lamb_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.lamb_update" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Update weights ws of some function, given the derivatives of some cost c with
respect to ws, [dc/dw for w in ws], by applying LAMB method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ws</strong> ‚Äì Weights of the function to be updated.</p></li>
<li><p><strong>dcdws</strong> ‚Äì Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> ‚Äì Learning rate(s), the rate(s) at which the weights should be updated relative to
the gradient.</p></li>
<li><p><strong>mw_tm1</strong> ‚Äì running average of the gradients, from the previous time-step.</p></li>
<li><p><strong>vw_tm1</strong> ‚Äì running average of second moments of the gradients, from the previous time-step.</p></li>
<li><p><strong>step</strong> ‚Äì training step</p></li>
<li><p><strong>beta1</strong> ‚Äì gradient forgetting factor (Default value = 0.9)</p></li>
<li><p><strong>beta2</strong> ‚Äì second moment of gradient forgetting factor (Default value = 0.999)</p></li>
<li><p><strong>epsilon</strong> ‚Äì divisor during adam update, preventing division by zero (Default value = 1e-7)</p></li>
<li><p><strong>max_trust_ratio</strong> ‚Äì The maximum value for the trust ratio. Default is 10.</p></li>
<li><p><strong>decay_lambda</strong> ‚Äì The factor used for weight decay. Default is zero.</p></li>
<li><p><strong>inplace</strong> ‚Äì Whether to perform the operation inplace, for backends which support inplace
variable updates, and handle gradients behind the scenes such as PyTorch. If the
update step should form part of a computation graph (i.e. higher order
optimization), then this should be set to False. Default is True, provided the
backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> ‚Äì Whether to stop the gradients of the variables after each gradient step.
Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>ret</em> ‚Äì The new function weights ws_new, following the LARS updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.lars_update">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">lars_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ws</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dcdws</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decay_lambda</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_gradients</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#lars_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.lars_update" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Update weights ws of some function, given the derivatives of some cost c with
respect to ws, [dc/dw for w in ws], by applying Layerwise Adaptive Rate Scaling
(LARS) method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ws</strong> ‚Äì Weights of the function to be updated.</p></li>
<li><p><strong>dcdws</strong> ‚Äì Derivates of the cost c with respect to the weights ws, [dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> ‚Äì Learning rate, the rate at which the weights should be updated relative to the
gradient.</p></li>
<li><p><strong>decay_lambda</strong> ‚Äì The factor used for weight decay. Default is zero.</p></li>
<li><p><strong>inplace</strong> ‚Äì Whether to perform the operation inplace, for backends which support inplace
variable updates, and handle gradients behind the scenes such as PyTorch. If the
update step should form part of a computation graph (i.e. higher order
optimization), then this should be set to False. Default is True, provided the
backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> ‚Äì Whether to stop the gradients of the variables after each gradient step.
Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>ret</em> ‚Äì The new function weights ws_new, following the LARS updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.optimizer_update">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">optimizer_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ws</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">effective_grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_gradients</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#optimizer_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.optimizer_update" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Update weights ws of some function, given the true or effective derivatives of
some cost c with respect to ws, [dc/dw for w in ws].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ws</strong> ‚Äì Weights of the function to be updated.</p></li>
<li><p><strong>effective_grads</strong> ‚Äì Effective gradients of the cost c with respect to the weights ws,
[dc/dw for w in ws].</p></li>
<li><p><strong>lr</strong> ‚Äì Learning rate(s), the rate(s) at which the weights should be updated relative to
the gradient.</p></li>
<li><p><strong>inplace</strong> ‚Äì Whether to perform the operation inplace, for backends which support inplace
variable updates, and handle gradients behind the scenes such as PyTorch. If the
update step should form part of a computation graph (i.e. higher order
optimization), then this should be set to False. Default is True, provided the
backend framework supports it.</p></li>
<li><p><strong>stop_gradients</strong> ‚Äì Whether to stop the gradients of the variables after each gradient step.
Default is True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>ret</em> ‚Äì The new function weights ws_new, following the optimizer updates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.set_with_grads">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">set_with_grads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">with_grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#set_with_grads"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.set_with_grads" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Summary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>with_grads</strong> ‚Äì </p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.stop_gradient">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">stop_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preserve_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#stop_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.stop_gradient" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Stops gradient computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> ‚Äì Array for which to stop the gradient.</p></li>
<li><p><strong>preserve_type</strong> ‚Äì Whether to preserve the input type (ivy.Variable or ivy.Array),
otherwise an array is always returned. Default is True.</p></li>
<li><p><strong>preserve_type</strong> ‚Äì bool, optional (Default value = True)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>ret</em> ‚Äì The same array x, but with no gradient information.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.unset_with_grads">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">unset_with_grads</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#unset_with_grads"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.unset_with_grads" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.variable">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">variable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#variable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.variable" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Creates a variable, which supports gradient computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Array</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">NativeArray</span></code>]) ‚Äì An ivy array.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><em>ret</em> ‚Äì An ivy variable, supporting gradient computation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.variable_data">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">variable_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#variable_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.variable_data" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Some backends wrap arrays in a dedicated variable class. For those frameworks,
this function returns that wrapped array. For frameworks which do not have a
dedicated variable class, the function returns the data passed in.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> ‚Äì An ivy variable.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>ret</em> ‚Äì The internal data stored by the variable</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ivy.functional.ivy.gradients.with_grads">
<span class="sig-prename descclassname">ivy.</span></span><span class="sig-name descname"><span class="pre">with_grads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">with_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ivy/functional/ivy/gradients.html#with_grads"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ivy.functional.ivy.gradients.with_grads" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Summary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>with_grads</strong> ‚Äì (Default value = None)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>ret</em></p>
</dd>
</dl>
</dd></dl>

<div class="toctree-wrapper compound">
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="general/get_num_dims.html" class="btn btn-neutral float-left" title="get_num_dims" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="gradients/with_grads.html" class="btn btn-neutral float-right" title="with_grads" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2022, Ivy Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>